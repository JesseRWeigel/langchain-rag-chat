# Chat with Your Docs (LangChain + RAG MVP)

This project is a minimal web app that lets you upload PDF or TXT documents and ask questions about their content. It processes the documents by splitting them into manageable text chunks, converts these chunks into embeddings using OpenAI's GPT-4 API, and then leverages a Chroma vector store to retrieve the most relevant context for answering your questions.

## Features

- Document Upload: Easily upload PDF and TXT files through a simple web interface.
- Document Processing: Extracts text using LangChain's document loaders and splits it into chunks via a recursive character text splitter.
- Embeddings & Vector Store: Converts text chunks into embeddings with OpenAIEmbeddings and stores them in a Chroma vector database.
- Question Answering: Uses a GPT-4 powered chat model to answer questions based on the document's content.

## Technologies Used

- Python 3.13: The programming language for the project.
- Streamlit: Framework for building interactive web apps with minimal code.
- LangChain: Provides document loaders, text splitters, chains, and more to simplify working with large language models.
- Chroma: A vector database used for storing and querying document embeddings.
- OpenAI GPT-4: The advanced language model that generates the responses.
- PyPDFLoader & TextLoader: Modules from LangChain that load and extract text from PDFs and TXT files.
- RecursiveCharacterTextSplitter: Splits documents into smaller chunks to work within the model's context limits.
- python-dotenv: Loads environment variables (e.g., your OpenAI API key) from a .env file.
- tiktoken: A tokenization library required by OpenAIEmbeddings.
- langchain-community: Additional community modules that extend LangChain functionality.

## Setup Instructions

1. Clone the Repository:

```bash
git clone <repository-url>
cd langchain-rag-chat
```

2. Create a Virtual Environment:

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows use: venv\Scripts\activate
```

3. Install Dependencies:
   All required packages are listed in the requirements.txt file. Install them by running:

```bash
pip install -r requirements.txt
```

4. Configure Environment Variables:
   Create a .env file in the project's root directory and add your OpenAI API key:

```env
OPENAI_API_KEY=your_openai_api_key_here
```

## Running the App

Launch the app using Streamlit:

```bash
streamlit run app.py
```

Then open your web browser and navigate to http://localhost:8501 to use the app. Upload a document, type your question, and see the answer generated by the system.

## How It Works

1. Document Loading:

   - The app uses PyPDFLoader for PDFs and TextLoader for TXT files to extract text from uploaded documents.

2. Text Splitting:

   - Extracted text is split into smaller chunks using RecursiveCharacterTextSplitter, which helps fit the model's context window.

3. Embeddings Generation:

   - Each text chunk is converted into a high-dimensional embedding using OpenAIEmbeddings. (This step requires the tiktoken package.)

4. Vector Store (Chroma):

   - The embeddings are stored in a Chroma vector database, making it quick to retrieve relevant document segments based on a query.

5. Question Answering:
   - When a question is asked, the app retrieves relevant document chunks from Chroma and passes them to GPT-4 through LangChain's RetrievalQA chain, which generates the answer.

## Project Structure

- `app.py`: Main application file that integrates document loading, processing, vector store creation, and the Streamlit UI.
- `.env`: File containing your environment variables (e.g., the OpenAI API key).
- `requirements.txt`: Lists all required Python packages.
- Other files and directories may be added as the project expands.
